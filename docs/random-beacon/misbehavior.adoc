= Attacks and misbehavior

== Types of participants

=== Honest

A honest actor will always follow the protocol.

If the return for participating honestly _R~h~_ is below the required rate of
return (eg. cost of capital, operating expenses), a honest participant will
abstain from participating, even if a misbehaving course of action is available
that would yield greater returns.

The protocol should avoid relying on any participants being honest.

=== Greedy

A greedy actor will want to maximize its return.

A greedy actor is willing to violate protocol and misbehave in arbitrary ways if
doing so has a positive expected value, but will not take action which reduces
their returns. A greedy actor does not care about the impact on other
participants unless they have entered a binding contract to share the profits.

A greedy participant will misbehave if and only if the return from doing so
_R~m~_ is equal or greater than the return from honest participation _R~h~_.

The protocol should be designed to be viable if most participants are greedy. It
is also to be expected that some greedy participants will have external
incentives; for example, manipulating a lottery to greatly increase the chance
of winning may be worth taking a punishment for.

=== Malicious

A malicious actor will seek to subvert or inflict damage on the network.

A malicious actor is willing to misbehave even at an immediate cost to itself,
at the pursuit of some ulterior motive in opposition to the intended functioning
of the network.

==== Adversary power

The main constraint on the malicious actor is the amount of resources it has
available _res~m~_, especially in relation to the resources of the well-behaved
participants _res~h~_. The ratio _power~m~ = res~m~ / res~h~_ determines the
_adversary power_ of the malicious actor. Because money can be used to buy
various forms of influence, it does not seem sensible to assume a malicious
actor would be limited to any particular type of influence (a fraction of
stakers only, with no miners, or a fraction of miners but no stakers). Instead,
a safer assumption is that the adversary can control a fraction of each specific
type of participant, determined by _power~m~ / (1 + power~m~)_. For example,
with _power~m~ = 0.5_ the adversary can control 33% of all miners, stakers, etc.

The protocol should be resistant to malicious participants, especially those of
low adversary power. Any protocol relying on a honest majority is vulnerable to
adversaries of _power~m~ >= 1_ (a 51% attack).

==== Control bootstrapping

_Control bootstrapping_ is the class of attacks a malicious actor can use to
increase its share of control over the network, and thus one of the most
significant. Preventing and limiting control bootstrapping attacks is extremely
important for protecting against malicious participants.

If an adversary with _power~m~_ power can cause _D~h~_ damage to the
well-behaved participants at a cost of _D~m~_ to itself, so that
_D~m~ / D~h~ < power~m~_, the attack would increase the attacker's adversary
power relative to the well-behaved participants:
_power'~m~ = (res~m~ - D~m~) / (res~h~ - D~h~) > power~m~_. 
The effectiveness of attack _A_ can be expressed as its _damage multiplier_:
_mult~A~ = D~h~ / D~m~_. Attack _A_ is useful to adversary _m_ if
_mult~A~ * power~m~ > 1_.

Such attacks can be used to "smoke out" honest and greedy participants by
rendering staking unprofitable without the external budget provided by the
malicious actor's ulterior motives. If an attack can be iterated enough times,
the well-behaved participants would run out of resources before the adversary.
Thus over time the adversary would be able to bootstrap a smaller degree of
control over the network into a larger one.

== Participant status

=== Well-behaved

A well-behaved participant is correctly following the protocol and sending the
expected messages at expected times.

=== Inactive

An inactive participant is one that has stopped sending the messages expected by
the protocol, but is otherwise well-behaved. An inactive participant does not
send invalid messages or otherwise actively undermine the protocol execution.

A participant can be rendered inactive due to internal, such as node
misconfiguration or downtime, or external factors, such as network problems or
DoS attacks.

Transient inactivity is expected to be the most common form of protocol
violation. Care should be taken not to punish such cases excessively harshly,
especially because, unlike other forms of misbehavior, an attacker can force
a participant to be inactive.

=== Misbehaving

A misbehaving participant has engaged in active protocol violations by sending
messages (broadcasting to other nodes, or publishing transactions on-chain) that
are invalid in either format or content.

Wherever feasible, misbehavior should be attributable and punishable. A third
party cannot force a non-compromised participant to misbehave attributably, and
thus any such misbehavior can be blamed on the participant who signed the bad
message.

==== Compromised participants

An otherwise well-behaved participant whose private key has leaked to a third
party can be made to misbehave by using the compromised key. This is impossible
to distinguish from straightforward misbehavior, as an attacker could just as
well set up a scheme that makes them appear legitimately compromised, but such
distinction is neither necessary nor desirable. Penalizing the stakers whose
operators have been compromised acts as an incentive to protect against such
compromise, and helps remove vulnerable operators from the network.

== Types of misbehavior

=== Incorrect signature share

Instead of broadcasting _Sig~m~_, _P~m~_ broadcasts a different, incorrect
value _Sig'~m~_ to other members of the signing group.

==== Attributability

*Full*

==== Viability

*Low*

Signature shares can be validated against the individual public key known from
DKG, and invalid shares ignored.

==== Mitigation

*None* (initially) to *full* (potentially)

Relies on DKG individual public key attributability

Relies on tracking beacon outputs signed by each group

Signing an incorrect value can be proven with the signature that doesn't
correspond to any previous beacon input, the member's public key, and its merkle
path to the DKG merkle root.


=== Selfish signing - greedy

Instead of revealing their signature share the operator waits to receive shares
from others, trying to delay others' signature reconstruction and to gain
the submitter reward.

==== Attributability

*Low*

A signature submission by a node that hasn't broadcast its own signature share
is evidence of selfish signing, but this can be circumvented by having the
submitter node broadcast its share normally while the adversary's other nodes
withhold theirs.

As the signature is reconstructed from shares it is not possible to tell which
nodes provided the shares for a given submission, and if the adversary
participates normally on one node its other nodes are indistinguishable from
offline nodes.

==== Viability

*Moderate* to *high*

The relationship of the derivative of the submitter's reward (grows over time)
vs the group rewards (declines over time) influences the viability of this
attack.

If the attack is successful at _T~submit~_, the adversary benefits by
_R~submitter~(T~submit~) - R~group~(T~submit~)_.
Because selfish signing results in the signature being submitted later than
it otherwise would at _T'~submit~_, the full viability of the greedy variant for
an adversary with _m_ nodes is governed by
__R~submitter~(T~submit~) + (m - 1) * R~group~(T~submit~) - m * R~group~(T'~submit~)_.

If _R~submitter~(T) > R~group~(T')_ for all _T, T'_, successful selfish signing
is always profitable when _m = 1_ and will remain profitable for larger _m_ if
_T_ and _T'_ are close enough.

If the selfish signing is unsuccessful, the operator will lose out on
_m * (R~group~(T'~submit~) - R~group~(T~submit~))_
which is small if _T_ is close to _T'_.

Additionally, delaying the submission deliberately can be profitable when
_dR~submitter~ / dT + (m - 1) * dR~group~ / dT > 0_ (always for _m = 1_).

If the system is modeled as a theoretical broadcast channel and the adversary
as rushing, the attack will always succeed. In a real system the probability of
success increases with _m_; a true linchpin adversary with
_m + n~inactive~ + n~disqualified~ > M_
will always succeed at selfish signing. Variability in message transmission
time reduces success chances as a different member may have their signature
reconstructed by the time _P~m~_ has received _H - m_ shares. Miner censorship
and DoS attacks on the other participants can improve chances of success.

==== Mitigation

*None* (initially) to *high to full* (potentially)

A fair exchange protocol or similar could be used to force operators to share
their signature shares such that any individual node only learns enough shares
to reconstruct the signature if all participants learn enough shares. In the
greedy form of the attack the adversary only cares about maximizing its own
profit, and thus it will prefer to reveal its signature share to others if not
doing so would result in an indefinite stalemate.

The cost of such a fair exchange as opposed to simple broadcast is lower group
rewards for all participants due to slower submission of results, but
participating in a suitable fair exchange protocol would be a dominant strategy.
TODO: Whether fair exchange protocols that are resistant to a rushing adversary
without a trusted third party are possible needs further investigation.


=== Selfish signing - malicious

A linchpin operator who can block others from completing the threshold
signature selfishly submits the signature at such time _T~submit~_ that their
own net reward
_R~submitter~(T~submit~) + (m - 1) * R~group~(T~submit~)_
is positive while everyone else suffers a late submission penalty:
_R~group~(T~submit~) < 0_.

This attack can be used to bootstrap dominance in the network.

==== Attributability

*Low*

A signature submission by a node that hasn't broadcast its own signature share
is evidence of selfish signing, but this can be circumvented by having the
submitter node broadcast its share normally while the adversary's other nodes
withhold theirs.

As the signature is reconstructed from shares it is not possible to tell which
nodes provided the shares for a given submission, and if the linchpin operator
participates normally on one node its other nodes are indistinguishable from
offline nodes.

==== Viability

*High* with sufficient adversary power;
can be augmented by DoS capabilities

The relationship of the derivative of the submitter's reward (grows over time)
vs the group rewards (declines over time) influences the viability of this
attack: an adversary with _m_ nodes can benefit absolutely when
_dR~submitter~ / dT + (m - 1) * dR~group~ / dT > 0_
; an adversary with 1 node will always benefit but is highly unlikely to become
linchpin, while an adversary with a large number of controlled nodes is less
likely to benefit but more likely to have linchpin status.

In relative terms this attack is always profitable, and regardless of _m_ there
always exists a _T~submit~_ such that
_R~group~(T~submit~) < 0_
but
_R~submitter~(T~submit~) + (m - 1) * R~group~(T~submit~) > 0_
and thus a sufficiently powerful and motivated adversary can use this attack to
bleed out honest participants

==== Mitigation

*None* (initially) to *limited* (potentially)

A malicious adversary may be willing to pay the opportunity costs of this attack
and thus mitigations are more difficult to come by than with a greedy adversary.

A fair exchange protocol does not work as well against a true linchpin
adversary, as the adversary could still behave extortionately and deny everybody
the beacon rewards unless it is allowed to become the submitter.

In such a case the honest nodes could attempt to extort in return, but due to
_m < h_ the marginal damage inflicted on the honest participants always exceeds
the damage on the malicious party in the case of an indefinite stalemate.

Additionally, identifying the presence of such an adversary is unlikely to be
possible except via circumstantial evidence.


=== Private key reveal

The opposite of selfish signing; the misbehaving staker reveals their individual
private key for the group's threshold signature with the intent to increase
their reward as a non-submitter by letting other stakers calculate their
signature share locally, and to let the staker be offline.

==== Attributability

*Full*

The corresponding public key is known by all members in the same group.

==== Viability

*Low*, requires extremely specific conditions to be profitable

If the cost of broadcast is high,
the cost of operating a well-functioning node high,
the reduction in group member rewards by increased time _dR~group~ / dT_ steep,
the number of nodes _m_ controlled by _P~m~_ high,
the probability of _P~m~_ otherwise getting submitter position low,
the difference in submitter and group rewards _R~submitter~(T) - R~group~(T)_ low,
the marginal improvement in submission time _T~submit~ - T'~submit~_ high,
and the (probability of being caught) times the (impact on token price) low,
it can be profitable to reveal one's individual signing key to other stakers so
that _Sig~m~_ can be calculated locally without need for _P~m~_'s participation.

==== Mitigation

*None* (initially) to *full* (potentially)

Relies on DKG individual public key attributability

Any member in the group can prove this happened by publishing a tattletale
message signed by the private key, containing the corresponding public key and
its merkle path generated from the DKG results. On-chain would then verify the
signature, and use the merkle path of the public key to verify that it belongs
to the accused group member. If a valid accusation is published, the accuser
would be rewarded and the misbehaving staker penalized.
